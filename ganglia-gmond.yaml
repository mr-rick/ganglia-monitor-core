apiVersion: v1
kind: Namespace
metadata:
  name: ganglia
---
apiVersion: v1
kind: Service
metadata:
  name: ganglia-gmond-infra
  namespace: ganglia
  labels:
    ganglia/role: infra
spec:
  selector:
    ganglia/role: infra
  ports:
  - name: gmond-tcp
    protocol: TCP
    port: 8649
    targetPort: 8649
  - name: gmond-udp
    protocol: UDP
    port: 8649
    targetPort: 8649
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ganglia-gmond
  namespace: ganglia
  labels:
    name: ganglia-gmond
spec:
  selector:
    matchLabels:
      name: ganglia-gmond
  template:
    metadata:
      labels:
        name: ganglia-gmond
    spec:    
      hostNetwork: true
      hostIPC: true
      hostPID: true
      terminationGracePeriodSeconds: 5
      initContainers:
      - image: alpine
        command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"]
        name: ganglia-init
        securityContext:
          privileged: true
      containers:
      - name: ganglia-gmond
        image: mrrick/ganglia-monitor-core:debian-3.7.2
        imagePullPolicy: Always
        env:
        - name: KUBERNETES_NODENAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: KUBERNETES_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: GANGLIA_CLUSTER_URL
          value: "https://peak6.net"
        command: ["/gmond-entrypoint.sh"]
        #command: ["sleep", "3600"]
        ports:
        - containerPort: 8649
        volumeMounts:
 #       - mountPath: /host/proc
 #         name: proc
 #         readOnly: false
 #       - mountPath: /host/sys
 #         name: sys
 #         readOnly: false
        - name: ganglia-gmond-conf
          mountPath: /tmp/ganglia-gmond
        securityContext:
          privileged: true
#          allowPrivilegeEscalation: true
      volumes:
#      - hostPath:
#          path: /proc
#        name: proc
#      - hostPath:
#          path: /sys
#        name: sys
      - name: ganglia-gmond-conf
        configMap:
          name: ganglia-gmond
          items:
          - key: gmond.conf
            path: gmond.conf.template
---
apiVersion: v1
data:
  gmond.conf: |+
    /* This configuration is as close to 2.5.x default behavior as possible
       The values closely match ./gmond/metric.h definitions in 2.5.x */
    globals {
      daemonize = no
      setuid = no
      user = nobody
      debug_level = 1
      max_udp_msg_len = 1472
      mute = no
      deaf = no
      allow_extra_data = yes
      host_dmax = 86400 /*secs. Expires (removes from web interface) hosts in 1 day */
      host_tmax = 20 /*secs */
      cleanup_threshold = 300 /*secs */
      gexec = no
      # By default gmond will use reverse DNS resolution when displaying your hostname
      # Uncommeting following value will override that value.
      override_hostname = ${KUBERNETES_NODENAME}
      # If you are not using multicast this value should be set to something other than 0.
      # Otherwise if you restart aggregator gmond you will get empty graphs. 60 seconds is reasonable
      send_metadata_interval = 60 /*secs */

    }

    /*
     * The cluster attributes specified will be used as part of the <CLUSTER>
     * tag that will wrap all hosts collected by this instance.
     */
    cluster {
      name = "${GANGLIA_CLUSTER_NAME}"
      owner = "${GANGLIA_CLUSTER_OWNER}"
      latlong = "${GANGLIA_CLUSTER_LATLONG}"
      url = "${GANGLIA_CLUSTER_URL}"
    }

    /* The host section describes attributes of the host, like the location */
    host {
      location = "${GANGLIA_NODE_ROLE}"
    }

    /* You can specify as many udp_recv_channels as you like as well. */
    udp_recv_channel {
      port = 8649
      #family = inet4
      #buffer = 10485760
    }

    /* Feel free to specify as many udp_send_channels as you like.  Gmond
       used to only support having a single channel */
    udp_send_channel {
      port = 8649
      host = ${GANGLIA_GMOND_UNICAST_HOST}
    }

    /* You can specify as many tcp_accept_channels as you like to share
       an xml description of the state of the cluster */
    tcp_accept_channel {
      port = 8649
      #family = inet4
      #gzip_output = no
    }

    /* Channel to receive sFlow datagrams */
    #udp_recv_channel {
    #  port = 6343
    #}

    /* Optional sFlow settings */
    #sflow {
    # udp_port = 6343
    # accept_vm_metrics = yes
    # accept_jvm_metrics = yes
    # multiple_jvm_instances = no
    # accept_http_metrics = yes
    # multiple_http_instances = no
    # accept_memcache_metrics = yes
    # multiple_memcache_instances = no
    #}

    /* Each metrics module that is referenced by gmond must be specified and
       loaded. If the module has been statically linked with gmond, it does
       not require a load path. However all dynamically loadable modules must
       include a load path. */
    modules {
      module {
        name = "core_metrics"
      }
      module {
        name = "cpu_module"
        path = "modcpu.so"
      }
      module {
        name = "disk_module"
        path = "moddisk.so"
      }
      module {
        name = "load_module"
        path = "modload.so"
      }
      module {
        name = "mem_module"
        path = "modmem.so"
      }
      module {
        name = "net_module"
        path = "modnet.so"
      }
      module {
        name = "proc_module"
        path = "modproc.so"
      }
      module {
        name = "sys_module"
        path = "modsys.so"
      }
    }

    /* The old internal 2.5.x metric array has been replaced by the following
       collection_group directives.  What follows is the default behavior for
       collecting and sending metrics that is as close to 2.5.x behavior as
       possible. */

    /* This collection group will cause a heartbeat (or beacon) to be sent every
       20 seconds.  In the heartbeat is the GMOND_STARTED data which expresses
       the age of the running gmond. */
    collection_group {
      collect_once = yes
      time_threshold = 20
      metric {
        name = "heartbeat"
      }
    }

    /* This collection group will send general info about this host*/
    collection_group {
      collect_every = 60
      time_threshold = 60
      metric {
        name = "cpu_num"
        title = "CPU Count"
      }
      metric {
        name = "cpu_speed"
        title = "CPU Speed"
      }
      metric {
        name = "mem_total"
        title = "Memory Total"
      }
      metric {
        name = "swap_total"
        title = "Swap Space Total"
      }
      metric {
        name = "boottime"
        title = "Last Boot Time"
      }
      metric {
        name = "machine_type"
        title = "Machine Type"
      }
      metric {
        name = "os_name"
        title = "Operating System"
      }
      metric {
        name = "os_release"
        title = "Operating System Release"
      }
      metric {
        name = "location"
        title = "Location"
      }
    }

    /* This collection group will send the status of gexecd for this host
       every 300 secs.*/
    /* Unlike 2.5.x the default behavior is to report gexecd OFF. */
    collection_group {
      collect_once = yes
      time_threshold = 300
      metric {
        name = "gexec"
        title = "Gexec Status"
      }
    }

    /* This collection group will collect the CPU status info every 20 secs.
       The time threshold is set to 90 seconds.  In honesty, this
       time_threshold could be set significantly higher to reduce
       unneccessary  network chatter. */
    collection_group {
      collect_every = 20
      time_threshold = 90
      /* CPU status */
      metric {
        name = "cpu_user"
        value_threshold = "1.0"
        title = "CPU User"
      }
      metric {
        name = "cpu_system"
        value_threshold = "1.0"
        title = "CPU System"
      }
      metric {
        name = "cpu_idle"
        value_threshold = "5.0"
        title = "CPU Idle"
      }
      metric {
        name = "cpu_nice"
        value_threshold = "1.0"
        title = "CPU Nice"
      }
      metric {
        name = "cpu_aidle"
        value_threshold = "5.0"
        title = "CPU aidle"
      }
      metric {
        name = "cpu_wio"
        value_threshold = "1.0"
        title = "CPU wio"
      }
      metric {
        name = "cpu_steal"
        value_threshold = "1.0"
        title = "CPU steal"
      }
      /* The next two metrics are optional if you want more detail...
         ... since they are accounted for in cpu_system.
      metric {
        name = "cpu_intr"
        value_threshold = "1.0"
        title = "CPU intr"
      }
      metric {
        name = "cpu_sintr"
        value_threshold = "1.0"
        title = "CPU sintr"
      }
      */
      /* The next two metrics are optional if you want more detail...
         ... since they are accounted for in cpu_user and cpu_nice.
      metric {
        name = "cpu_guest"
        value_threshold = "1.0"
        title = "CPU guest"
      }
      metric {
        name = "cpu_gnice"
        value_threshold = "1.0"
        title = "CPU gnice"
      }
      */
    }

    collection_group {
      collect_every = 20
      time_threshold = 90
      /* Load Averages */
      metric {
        name = "load_one"
        value_threshold = "1.0"
        title = "One Minute Load Average"
      }
      metric {
        name = "load_five"
        value_threshold = "1.0"
        title = "Five Minute Load Average"
      }
      metric {
        name = "load_fifteen"
        value_threshold = "1.0"
        title = "Fifteen Minute Load Average"
      }
    }

    /* This group collects the number of running and total processes */
    collection_group {
      collect_every = 80
      time_threshold = 950
      metric {
        name = "proc_run"
        value_threshold = "1.0"
        title = "Total Running Processes"
      }
      metric {
        name = "proc_total"
        value_threshold = "1.0"
        title = "Total Processes"
      }
    }

    /* This collection group grabs the volatile memory metrics every 40 secs and
       sends them at least every 180 secs.  This time_threshold can be increased
       significantly to reduce unneeded network traffic. */
    collection_group {
      collect_every = 40
      time_threshold = 180
      metric {
        name = "mem_free"
        value_threshold = "1024.0"
        title = "Free Memory"
      }
      metric {
        name = "mem_shared"
        value_threshold = "1024.0"
        title = "Shared Memory"
      }
      metric {
        name = "mem_buffers"
        value_threshold = "1024.0"
        title = "Memory Buffers"
      }
      metric {
        name = "mem_cached"
        value_threshold = "1024.0"
        title = "Cached Memory"
      }
      metric {
        name = "swap_free"
        value_threshold = "1024.0"
        title = "Free Swap Space"
      }
    }

    collection_group {
      collect_every = 40
      time_threshold = 300
      metric {
        name = "bytes_out"
        value_threshold = 4096
        title = "Bytes Sent"
      }
      metric {
        name = "bytes_in"
        value_threshold = 4096
        title = "Bytes Received"
      }
      metric {
        name = "pkts_in"
        value_threshold = 256
        title = "Packets Received"
      }
      metric {
        name = "pkts_out"
        value_threshold = 256
        title = "Packets Sent"
      }
    }

    /* Different than 2.5.x default since the old config made no sense */
    collection_group {
      collect_every = 1800
      time_threshold = 3600
      metric {
        name = "disk_total"
        value_threshold = 1.0
        title = "Total Disk Space"
      }
    }

    collection_group {
      collect_every = 40
      time_threshold = 180
      metric {
        name = "disk_free"
        value_threshold = 1.0
        title = "Disk Space Available"
      }
      metric {
        name = "part_max_used"
        value_threshold = 1.0
        title = "Maximum Disk Space Used"
      }
    }

    include ("/etc/ganglia/conf.d/*.conf")

    collection_group {
      collect_every = 40
      time_threshold = 180
      metric {
        name = "mem_available"
        value_threshold = "1024.0"
        title = "Available Memory"
      }
      metric {
        name = "mem_sreclaimable"
        value_threshold = "1024.0"
        title = "Slab Memory Reclaimable"
      }
    }
kind: ConfigMap
metadata:
  name: ganglia-gmond
  namespace: ganglia
